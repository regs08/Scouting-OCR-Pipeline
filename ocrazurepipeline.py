# -*- coding: utf-8 -*-
"""OCRAzurePipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10ZEyzgozhm0zlJVwBet491lTnZ-xuKqS

Assumptions:
* 1 values are lines and will be treated as 0s "selected" and "unselected" are mislabeled lines and will be treated as 0s

To take into account:
  * Only tested on one sheet.
  * Different hand writing
  * 'on the fly adjustments'
  * time it takes to enter manually vs. time it takes to check
    - compare this to accuracy
  * Acceptable error threshold?

Further work:
  * Normalize data entries e.g increments of 5
  * Fine tune a model (need to look elsewhere than Azure)
"""

import os

# Store your Azure credentials
os.environ["AZURE_FORM_RECOGNIZER_ENDPOINT"] = "https://table123.cognitiveservices.azure.com/"
os.environ["AZURE_FORM_RECOGNIZER_KEY"] = "FVsxOTknqJ0NDyeFy3EEAZ6p5BlsxmiOxkTiZUM9pY5LdPG31rOFJQQJ99BCACYeBjFXJ3w3AAALACOG9x8f"

!pip install azure-ai-formrecognizer

# Import Azure SDKs
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential

# Authenticate
endpoint = os.environ["AZURE_FORM_RECOGNIZER_ENDPOINT"]
key = os.environ["AZURE_FORM_RECOGNIZER_KEY"]
client = DocumentAnalysisClient(endpoint=endpoint, credential=AzureKeyCredential(key))

image_path = "/content/test.png"
# Read image bytes
with open(image_path, "rb") as f:
    image_data = f.read()

# Analyze document using the prebuilt-layout model (good for tables)
poller = client.begin_analyze_document("prebuilt-layout", document=image_data)
result = poller.result()

# Extract tables
import pandas as pd

for idx, table in enumerate(result.tables):
    print(f"\nTable {idx + 1}")
    table_data = [["" for _ in range(table.column_count)] for _ in range(table.row_count)]

    for cell in table.cells:
        table_data[cell.row_index][cell.column_index] = cell.content


# Create the DataFrame
df = pd.DataFrame(table_data)

# Promote the first row to header
df.columns = df.iloc[0]         # Set first row as column headers
df = df[1:]                     # Drop the now-header row from data
df.reset_index(drop=True, inplace=True)  # Reset row index    display(df)
df.head()

# gets rid of known unwanted values

def clean_cell(val):
    if isinstance(val, str):
        val_clean = val.strip().lower()
        # Sample Cleaning
        # :string: is an emoji.. The ones and slashes are from the NA entries
        if val_clean in [":selected:", ":unselected:", "/", "1"]:
            return 0
        try:
            # Convert to float first to handle decimals in string form
            float_val = float(val_clean)
            int_val = int(float_val)

            # If it's equivalent to 1, treat as unwanted
            return 0 if int_val == 1 else int_val
        except ValueError:
            return 0

    elif isinstance(val, (int, float)):
        int_val = int(val)
        return 0 if int_val == 1 else int_val

    return val

# Define columns to exclude from cleaning
exclude_cols = ["date", "disease", 'panel']

# Create a lowercase version of the column names for matching
cols_to_clean = [col for col in df.columns if str(col).strip().lower() not in exclude_cols]

# Apply cleaning only to selected columns
df_cleaned = df.copy()

for col in cols_to_clean:
    df_cleaned[col] = df_cleaned[col].apply(clean_cell)

display(df_cleaned)

import csv

# checks formating of GT data
expected_num_cols = None
ground_truth_path = "/content/ground_truth_sliced.csv"
with open(ground_truth_path, encoding="ISO-8859-1") as f:
    reader = csv.reader(f)
    for i, row in enumerate(reader, 1):
        if expected_num_cols is None:
            expected_num_cols = len(row)
        if len(row) != expected_num_cols:
            print(f"‚ùó Row {i} has {len(row)} columns, expected {expected_num_cols}")
            print(f"üîç Content: {row}")

def compare_columns(df1, df2, exclude_cols=None):
    exclude_cols = exclude_cols or []

    # Normalize excluded columns
    exclude_normalized = [col.strip().lower() for col in exclude_cols]

    # Filter out excluded columns
    df1_cols = [col for col in df1.columns if col.strip().lower() not in exclude_normalized]
    df2_cols = [col for col in df2.columns if col.strip().lower() not in exclude_normalized]

    # Normalized versions for comparison
    df1_normalized = {col.strip().lower(): col for col in df1_cols}
    df2_normalized = {col.strip().lower(): col for col in df2_cols}

    cols1 = set(df1_normalized.keys())
    cols2 = set(df2_normalized.keys())

    extra_keys_in_df1 = cols1 - cols2
    missing_keys_in_df1 = cols2 - cols1
    common_keys = cols1 & cols2

    print("üìä Column Comparison:")
    print(f"  üî¢ Columns in OCR (after excluding): {len(df1_cols)}")
    print(f"  üî¢ Columns in Ground Truth (after excluding): {len(df2_cols)}")

    extra_columns = [(df1_normalized[key], df1.columns.get_loc(df1_normalized[key])) for key in extra_keys_in_df1]
    missing_columns = [(df2_normalized[key], df2.columns.get_loc(df2_normalized[key])) for key in missing_keys_in_df1]

    if extra_columns:
        print(f"  ‚ûï Extra columns in OCR: {extra_columns}")
    if missing_columns:
        print(f"  ‚ûñ Missing columns in OCR: {missing_columns}")
    if not extra_columns and not missing_columns and len(df1_cols) == len(df2_cols):
        print("  ‚úÖ Columns match exactly.")

    # Common columns with index info
    common_columns_info = []
    for key in sorted(common_keys):
        col_df1 = df1_normalized[key]
        col_df2 = df2_normalized[key]
        idx_df1 = df1.columns.get_loc(col_df1)
        idx_df2 = df2.columns.get_loc(col_df2)
        common_columns_info.append((col_df1, idx_df1, idx_df2))

    return {
        "ocr_col_count": len(df1_cols),
        "truth_col_count": len(df2_cols),
        "extra_columns": extra_columns,
        "missing_columns": missing_columns,
        "common_columns": common_columns_info
    }

def align_column_names_by_index(df, extra_columns, missing_columns):
    """
    Renames extra columns in the DataFrame to match missing column names if their indices align.

    Parameters:
        df (pd.DataFrame): The DataFrame to modify.
        extra_columns (list of tuples): (column_name, index) for extra columns in df.
        missing_columns (list of tuples): (expected_column_name, index) for columns missing in df.

    Returns:
        pd.DataFrame: Updated DataFrame with renamed columns.
    """
    col_list = list(df.columns)

    # Build a mapping where index matches
    index_map = {}
    for (extra_name, extra_idx) in extra_columns:
        for (missing_name, missing_idx) in missing_columns:
            if extra_idx == missing_idx:
                index_map[extra_idx] = (extra_name, missing_name)

    # Apply renaming
    for idx, (old_name, new_name) in index_map.items():
        print(f"üîÑ Renaming column '{old_name}' at index {idx} ‚Üí '{new_name}'")
        col_list[idx] = new_name

    # Assign new column list
    df.columns = col_list
    return df

def compare_row_counts(df1, df2):
    len1, len2 = len(df1), len(df2)

    print("\nüìè Row Count Comparison:")
    print(f"  OCR Rows: {len1}")
    print(f"  Ground Truth Rows: {len2}")

    if len1 != len2:
        print("  ‚ö†Ô∏è Row count mismatch!")
    else:
        print("  ‚úÖ Row counts match.")

    return min(len1, len2)  # use the minimum for safe comparison


def compare_values(df1, df2, columns, max_rows=None, verbose=False):
    print("\nüîç Value Comparison:")
    mismatches = []
    matches = 0

    # Reset index for clean access
    df1 = df1[columns].reset_index(drop=True).fillna("").astype(str)
    df2 = df2[columns].reset_index(drop=True).fillna("").astype(str)

    row_limit = max_rows or min(len(df1), len(df2))
    total_cells = row_limit * len(columns)

    for i in range(row_limit):
        for col in columns:
            val1 = df1.at[i, col]
            val2 = df2.at[i, col]
            if val1 != val2:
                mismatches.append({
                    "row": i,
                    "column": col,
                    "ocr_value": val1,
                    "truth_value": val2
                })
            else:
                matches += 1

    accuracy = matches / total_cells * 100 if total_cells else 0

    print(f"  ‚úÖ Matching Cells: {matches} / {total_cells}")
    print(f"  üéØ Accuracy: {accuracy:.2f}%")

    if verbose and mismatches:
        print("\n  ‚ùå Mismatched values:")
        for m in mismatches:
            print(f"    Row {m['row']} | Column '{m['column']}': OCR='{m['ocr_value']}' ‚Üí Truth='{m['truth_value']}'")

    return {
        "accuracy": accuracy,
        "total_cells": total_cells,
        "matches": matches,
        "mismatches": mismatches
    }

def run_comparison(ocr_df, ground_truth_path, exclude_cols=None, verbose=False):
    ground_truth_df = pd.read_csv(ground_truth_path, dtype=str, encoding="ISO-8859-1").fillna("")
    ocr_df = ocr_df.fillna("").astype(str)

    # Step 1: Compare columns
    common_columns = compare_columns(ocr_df, ground_truth_df, exclude_cols=exclude_cols)

    # Step 2: Compare row count
    row_limit = compare_row_counts(ocr_df, ground_truth_df)

    # Step 3: Compare values
    result = compare_values(
        df1=ocr_df,
        df2=ground_truth_df,
        columns=common_columns,
        max_rows=row_limit,
        verbose=verbose
    )

    return result

ocr_df = df_cleaned.copy()
ground_truth_path="/content/ground_truth_sliced.csv"
ground_truth_df = pd.read_csv(ground_truth_path, dtype=str, encoding="ISO-8859-1").fillna("")

print("#####\nComparing cleaned DF to GT DF\n#####")
compare_col_data = compare_columns(ocr_df, ground_truth_df)
extra_columns = compare_col_data["extra_columns"]
missing_columns = compare_col_data["missing_columns"]

## if the columns have the same index the GT value is assigned to the OCR DF
align_column_names_by_index(ocr_df, extra_columns, missing_columns)

print("#####\nComparing reindexed DF to GT DF\n#####")
compare_col_data = compare_columns(ocr_df, ground_truth_df)

#Row counts
compare_row_counts(ocr_df, ground_truth_df)

data_cols = ['L1', 'L2', 'L3', 'L4', 'L5', 'L6', 'L7', 'L8', 'L9', 'L10', 'L11', 'L12', 'L13', 'L14', 'L15', 'L16', 'L17', 'L18', 'L19', 'L20']
index_cols = ['√Ø¬ª¬ødate', 'row', 'panel', 'disease']

print(f"\n#####\nComparing Data Columns: {data_cols}\n#####")
data_comparison = compare_values(ocr_df, ground_truth_df, columns=data_cols, verbose=True)

print(f"#####\nComparing Index Columns: {index_cols}\n#####")
index_comparison = compare_values(ocr_df, ground_truth_df, columns=index_cols, verbose=True)

!pip install openpyxl

# Filenames
excel_filename = "cleaned_table.xlsx"
csv_filename = "cleaned_table.csv"
pkl_filename = "cleaned_table.pkl"

# Save files
df_cleaned.to_excel(excel_filename, index=False)
df_cleaned.to_csv(csv_filename, index=False)
df_cleaned.to_pickle(pkl_filename)

# Download the files (Colab)
from google.colab import files
files.download(excel_filename)
files.download(csv_filename)
files.download(pkl_filename)